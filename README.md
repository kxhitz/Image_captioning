# Image Captioning with VGG16 and LSTM

This project implements a deep learning model to automatically generate descriptive captions for images. It uses an encoder-decoder architecture, leveraging a pre-trained Convolutional Neural Network (CNN) to extract image features and a Long Short-Term Memory (LSTM) network to generate the corresponding text sequence.

The model is trained on the Flickr8k dataset.

---

## Table of Contents
- [Project Overview](#project-overview)
- [Model Architecture](#model-architecture)
- [Dataset](#dataset)
- [Dependencies](#dependencies)
- [How to Use](#how-to-use)
- [Results](#results)
- [Future Improvements](#future-improvements)

---

## Project Overview

The goal of this project is to create a model that can "see" an image and generate a human-like textual description. This is achieved through the following steps:

1.  **Image Feature Extraction:** A pre-trained VGG16 model is used to extract a rich feature vector (a 4096-dimensional vector) from each image. This process, known as transfer learning, leverages the power of a model already trained on the vast ImageNet dataset.
2.  **Text Preprocessing:** The captions from the Flickr8k dataset are cleaned and tokenized. This involves converting text to lowercase, removing punctuation, and adding special 'startseq' and 'endseq' tokens to signify the beginning and end of a caption.
3.  **Model Training:** An encoder-decoder model is built. The image features (encoder output) and the tokenized text sequences are fed into a decoder (LSTM network) to teach the model how to generate words in sequence based on the image's content.
4.  **Caption Generation (Inference):** Given a new image, its features are extracted and fed into the trained decoder. The model then generates a caption word by word until it produces the 'endseq' token.

---

## Model Architecture

The model is an encoder-decoder network that combines a CNN with a Recurrent Neural Network (RNN).

### Encoder (Image Model)
-   **Model:** VGG16 pre-trained on ImageNet.
-   **Process:** The final classification layer of VGG16 is removed. The output of the second-to-last fully connected layer (`fc2`), which is a 4096-dimensional vector, is used as the image feature embedding.
-   **Output:** This feature vector is passed through a Dense layer to reduce its dimensionality to 256, which serves as the initial input for the decoder.

### Decoder (Language Model)
-   **Model:** Long Short-Term Memory (LSTM) network.
-   **Process:**
    1.  The text captions are converted into integer sequences. An Embedding layer then converts these integers into dense vectors of size 256.
    2.  An LSTM layer with 256 units processes the sequence of word embeddings.
    3.  The output from the Image Model (encoder) and the Language Model (LSTM) are added together.
    4.  This combined vector is passed through a final Dense layer with a `softmax` activation function, which predicts the probability distribution for the next word in the sequence over the entire vocabulary.

![Model Diagram](https://i.imgur.com/Wp7p3eP.png)
*(This is the diagram generated by `plot_model` in your notebook. You should save the image and add it to your repository for this link to work.)*

---

## Dataset

This project uses the **Flickr8k dataset**.
-   **Images:** 8,091 JPG images.
-   **Captions:** 5 captions for each image.
-   The dataset can be downloaded from Kaggle: [Flickr8k Dataset](https://www.kaggle.com/datasets/adityajn105/flickr8k)

---

## Dependencies

The project is built using Python 3 and the following major libraries. You can install them using pip.

```bash
pip install tensorflow numpy nltk Pillow matplotlib
```

-   TensorFlow (Keras)
-   NLTK (for BLEU score evaluation)
-   NumPy
-   Pillow (PIL)
-   Matplotlib (for visualization)
-   tqdm (for progress bars)

It is recommended to create a `requirements.txt` file for easier installation.

---

## How to Use

1.  **Clone the Repository:**
    ```bash
    git clone [https://github.com/your-username/your-repository-name.git](https://github.com/your-username/your-repository-name.git)
    cd your-repository-name
    ```

2.  **Download the Dataset:**
    - Download the Flickr8k dataset from the link above.
    - Create a directory (e.g., `flickr8k`) and place the `Images` folder and `captions.txt` file inside it.
    - Update the `BASE_DIR` variable in the notebook to point to this directory.

3.  **Run the Jupyter Notebook (`image-captioning.ipynb`):**
    - The notebook is structured to be run sequentially.
    - **Feature Extraction:** The first part extracts features from all images and saves them to `features.pkl`. This is time-consuming but only needs to be done once.
    - **Model Training:** The notebook will then preprocess the text, build the model, and train it for 20 epochs. The trained model is saved as `best_model.h5`.
    - **Inference & Evaluation:** The final cells use the trained model to generate captions for test images and evaluate the performance using the BLEU score.

---

## Results

The model's performance is evaluated using the BLEU (Bilingual Evaluation Understudy) score, which measures the similarity between the machine-generated caption and the reference human captions.

-   **BLEU-1 Score:** 0.533
-   **BLEU-2 Score:** 0.307

### Sample Predictions:

**Image 1:** `1002674143_1b742ab4b8.jpg`
-   **Actual Captions:**
    -   `startseq little girl covered in paint sits in front of painted rainbow with her hands in bowl endseq`
    -   `startseq little girl is sitting in front of large painted rainbow endseq`
    -   `startseq small girl in the grass plays with fingerpaints in front of white canvas with rainbow on it endseq`
-   **Predicted Caption:**
    -   `startseq little girl in pigtails is sitting in the grass with fingerpaints in her head endseq`

**Image 2:** `102351840_323e3de834.jpg`
-   **Actual Captions:**
    -   `startseq man drilling hole in the ice endseq`
    -   `startseq man is drilling through the frozen ice of pond endseq`
    -   `startseq person in the snow drilling hole in the ice endseq`
-   **Predicted Caption:**
    -   `startseq two skateboarders play in the water endseq`

---

## Future Improvements
-   **Use a More Advanced Encoder:** Replace VGG16 with a more modern architecture like InceptionV3 or ResNet50 for potentially better feature extraction.
-   **Implement Attention Mechanism:** An attention mechanism would allow the decoder to focus on specific parts of the image when generating different words in the caption, which often leads to more accurate and contextually relevant descriptions.
-   **Use Beam Search:** Instead of greedy search (picking the most likely word at each step), beam search can be implemented during inference to find a more optimal overall sequence of words.
-   **Train on a Larger Dataset:** For better generalization, the model could be trained on a larger dataset like Flickr30k or MS COCO.
